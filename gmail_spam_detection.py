# -*- coding: utf-8 -*-
"""Gmail spam detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F9iymgyYCq8sluHsN34lseUsQAGQ-LgE
"""

pip install pandas

pip install nltk

import nltk

nltk.download('wordnet')

#Loading Dataset
import pandas as pd

#instead of pandas in the whole program we wll only write "pd"

dt=pd.read_csv("spam.csv",encoding="unicode_escape")

dt.head(10)
#head() function is used to selct the "n " rows. By default the n here is 5. You can change this n to number of rows u want.

#we are changin ham to 0 and spam to 1
dt['spam']=dt['type'].map({'spam':1,'ham':0}).astype(int)



dt.head(10)

#Print the number of columns
print("Columns in the given data")
for col in dt.columns:
  print(col)

#print the number of rows in the dataset
d=len(dt['type'])
print("No. of rows is",d)

dt.text[4]

#tokenisation
#i am goin to create a functio  which will covert the text to tokens

def tokeniser(text):
  return text.split()

  #split function is used to tokenise

dt['text']=dt['text'].apply(tokeniser)

dt.text[4]

#Stemming
dt.[4]

dt.text[4] #before

from nltk.stem.snowball import SnowballStemmer

porter= SnowballStemmer("english",ignore_stopwords=False)
#we should check the documentation of SnowballSetemmer

def stem_it(text):
  return [porter.stem(word) for word in text]

#applying to the whole file
dt['text']=dt['text'].apply(stem_it)

dt.text[4]

#Lemmetization
from nltk.stem import WordNetLemmatizer

lemmatizer=WordNetLemmatizer()

def lemmit(text):
    return [lemmatizer.lemmatize(word , pos='a')for word in text]

dt.text[4]

nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words=stopwords.words("english")

def stop_it(text):
  review=[word for word in text if not word in stop_words]
  return review

dt['text']=dt['text'].apply(stop_it)

dt.text[4]

dt.head()

dt['text']=dt['text'].apply(''.join)
#change list to normal

dt.head()

#vectorization

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf=TfidfVectorizer()
y=dt.spam.values
x=tfidf.fit_transform(dt['text'])

from sklearn.model_selection import train_test_split
x_train,x_text,y_train,y_text=train_test_split(x,y,random_state=1,test_size=0.2,shuffle=False)

from sklearn.linear_model import LogisticRegression
clf=LogisticRegression()
clf.fit(x_train,y_train)
y_pred=clf.predict(x_text)

from sklearn.metrics import accuracy_score
acc_log = accuracy_score(y_pred,y_text)*100
print("Accuracy",acc_log)

























